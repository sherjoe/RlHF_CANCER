{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning Human Feedback for Cancer Cell Detection through Dyanmic Policies with (M2 Max)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤–  <- Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Allowing a certain number of phases of iterative human feedback and different algorithms identifying RGB values, shape, texture, extraneous information, etc., can help shape the environmental reward signal and aid the agent to choose exploration over exploitation and vice versa.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage and Preprocess image dataset for screening ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-train/BTTAIxNYBG-train' # Get better datasets and categorical classfications\n",
    "valid_dir = '/kaggle/input/bttai-nybg-2024/BTTAIxNYBG-validation/BTTAIxNYBG-validation'\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    rescale=1./255, # Rescale pixel values to [0, 1]\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[1.0, 1.1], # Adjust brightness randomly within the range [0.8, 1.2]\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "valid_ds = valid_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup RL Environment ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces # hope this is open source\n",
    "import numpy as np\n",
    "import cv2  # For image processing\n",
    "\n",
    "class CancerCellDetectionEnv(gym.Env):\n",
    "    def __init__(self, images, labels):\n",
    "        super(CancerCellDetectionEnv, self).__init__()\n",
    "        self.images = images  # List of images\n",
    "        self.labels = labels  # Corresponding labels\n",
    "        self.current_image_index = 0\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 100  # Define maximum steps per episode\n",
    "        \n",
    "        # Define action and observation space\n",
    "        # Actions: 0 = non-cancerous, 1 = cancerous\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        # Observations: Image patches (e.g., 64x64x3)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(64, 64, 3), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_image_index = np.random.randint(len(self.images))\n",
    "        self.current_step = 0\n",
    "        self.current_image = self.images[self.current_image_index]\n",
    "        self.current_label = self.labels[self.current_image_index]\n",
    "        \n",
    "        # Return initial observation (e.g., first image patch)\n",
    "        initial_observation = self.get_observation()\n",
    "        return initial_observation\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action and calculate reward\n",
    "        done = False\n",
    "        reward = self.calculate_reward(action)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "        \n",
    "        # Get the next observation\n",
    "        observation = self.get_observation()\n",
    "        \n",
    "        return observation, reward, done, {}\n",
    "\n",
    "    def get_observation(self):\n",
    "        # Extract the current patch or ROI from the image\n",
    "        # Here we just return the whole image for simplicity\n",
    "        return cv2.resize(self.current_image, (64, 64))\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        # Calculate reward based on the action and ground truth\n",
    "        correct_label = self.current_label  # For simplicity, assume label is for the whole image\n",
    "        if action == correct_label:\n",
    "            return 1  # Positive reward for correct classification\n",
    "        else:\n",
    "            return -1  # Negative reward for incorrect classification\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Optionally implement render method to visualize the environment\n",
    "        cv2.imshow('Cancer Cell Detection', self.current_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == '__main__':\n",
    "    images = [cv2.imread('path_to_image1.jpg'), cv2.imread('path_to_image2.jpg')]  # List of images\n",
    "    labels = [0, 1]  # Corresponding labels (e.g., 0 = non-cancerous, 1 = cancerous)\n",
    "    \n",
    "    env = CancerCellDetectionEnv(images, labels)\n",
    "    \n",
    "    for episode in range(5):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action for testing\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            env.render()\n",
    "        \n",
    "        print(f'Episode {episode + 1}: Total Reward = {total_reward}')\n",
    "    \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization for RLHF Adpative/Dynamic Policies ~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Define the policy network (CNN)\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * input_shape[1] * input_shape[2], 256)\n",
    "        self.fc2 = nn.Linear(256, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Define the value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * input_shape[1] * input_shape[2], 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Define the PPO algorithm\n",
    "class PPOAgent:\n",
    "    def __init__(self, policy_net, value_net, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.optimizer_policy = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.optimizer_value = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "\n",
    "    def select_action(self, state):\n",
    "        logits = self.policy_net(state)\n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy()\n",
    "\n",
    "    def update(self, trajectories):\n",
    "        # Compute advantages and returns\n",
    "        returns, advantages = self.compute_advantages(trajectories)\n",
    "        # Update policy and value networks\n",
    "        for _ in range(10):  # Update 10 times for stability\n",
    "            for state, action, log_prob, advantage, ret in trajectories:\n",
    "                new_log_prob, entropy = self.policy_net(state)\n",
    "                ratio = torch.exp(new_log_prob - log_prob)\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantage\n",
    "                policy_loss = -torch.min(surr1, surr2) - 0.01 * entropy\n",
    "                self.optimizer_policy.zero_grad()\n",
    "                policy_loss.mean().backward()\n",
    "                self.optimizer_policy.step()\n",
    "\n",
    "                value_loss = (self.value_net(state) - ret) ** 2\n",
    "                self.optimizer_value.zero_grad()\n",
    "                value_loss.mean().backward()\n",
    "                self.optimizer_value.step()\n",
    "\n",
    "    def compute_advantages(self, trajectories):\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        for t in reversed(range(len(trajectories))):\n",
    "            state, action, reward, next_state, done = trajectories[t]\n",
    "            if done:\n",
    "                ret = reward\n",
    "            else:\n",
    "                ret = reward + self.gamma * self.value_net(next_state).detach()\n",
    "            returns.insert(0, ret)\n",
    "            advantage = ret - self.value_net(state).detach()\n",
    "            advantages.insert(0, advantage)\n",
    "        return returns, advantages\n",
    "\n",
    "# Training Loop\n",
    "input_shape = (3, 64, 64)  # Example input shape (channels, height, width)\n",
    "num_actions = 2  # Example number of actions (cancer cell or not)\n",
    "\n",
    "policy_net = PolicyNetwork(input_shape, num_actions)\n",
    "value_net = ValueNetwork(input_shape)\n",
    "ppo_agent = PPOAgent(policy_net, value_net)\n",
    "\n",
    "for epoch in range(1000):  # Number of epochs\n",
    "    trajectories = []  # Collect trajectories (state, action, reward, next_state, done)\n",
    "    for _ in range(100):  # Number of episodes per epoch\n",
    "        state = env.reset()  # Initialize the environment\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob, entropy = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            trajectories.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "    ppo_agent.update(trajectories)\n",
    "    # Incorporate human feedback -- Sher (im cooked)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
